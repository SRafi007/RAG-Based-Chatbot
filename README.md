# RAG-based Chatbot with Hybrid Retrieval

A production-ready chatbot system for company policy queries using hybrid RAG (Retrieval-Augmented Generation) with LangGraph orchestration.

---

## ğŸŒŸ Features

- **Hybrid RAG**: Combines sparse (BM25) and dense (semantic) retrieval for superior accuracy
- **Semantic Chunking**: Preserves document structure with heading-based splitting
- **Multi-Agent System**: LangGraph orchestration with plug-and-play architecture
- **Dual Storage**: Redis for fast access, PostgreSQL for persistence
- **Smart Classification**: Filters off-topic queries automatically
- **Streaming Support**: Real-time response generation
- **Rich Metadata**: Full attribution with section names and sources

---

## ğŸ—ï¸ Architecture

```
User Query
    â†“
[DomainGuard] â†’ Classify: policy-related or off-topic
    â†“
[RetrieverAgent] â†’ Hybrid RAG (Elasticsearch BM25 + Pinecone Semantic)
    â†“
[LLM Reranker] â†’ Improve precision
    â†“
[SummarizerAgent] â†’ Generate response with context
    â†“
Response with citations
```

**Technology Stack**:
- **LLM**: Google Gemini 2.5 Flash
- **Orchestration**: LangGraph (state-based workflow)
- **Sparse Retrieval**: Elasticsearch with BM25
- **Dense Retrieval**: Pinecone with OpenAI embeddings
- **API**: FastAPI
- **STM**: Redis
- **LTM**: PostgreSQL

---

## ğŸ“‹ Documentation

### Getting Started
- ğŸš€ **[CLOUD_SETUP_GUIDE.md](CLOUD_SETUP_GUIDE.md)** - Complete guide for setting up Elasticsearch, Pinecone, and OpenAI (for beginners)
- ğŸ“‹ **[QUICK_SETUP_REFERENCE.md](QUICK_SETUP_REFERENCE.md)** - Quick reference card with commands and troubleshooting

### Technical Documentation
- ğŸ›ï¸ **[ARCHITECTURE.md](ARCHITECTURE.md)** - System architecture and design decisions
- ğŸ“Š **[SYSTEM_OVERVIEW.md](SYSTEM_OVERVIEW.md)** - Component breakdown and data flow
- ğŸ”— **[KB_INTEGRATION.md](KB_INTEGRATION.md)** - Knowledge base pipeline integration guide
- ğŸ“š **[KB_SEMANTIC_CHUNKING.md](KB_SEMANTIC_CHUNKING.md)** - Semantic chunking strategy explained

### Pipeline Documentation
- ğŸ“– **[kb_pipeline/README.md](kb_pipeline/README.md)** - KB pipeline detailed documentation

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Redis (cloud or local)
- PostgreSQL (Supabase recommended)
- Google Gemini API key

### 1. Installation

```bash
# Clone repository
git clone <your-repo-url>
cd RAG-based-Chatbot

# Create virtual environment
python -m venv rag_env

# Activate virtual environment
# Windows:
rag_env\Scripts\activate
# Linux/Mac:
source rag_env/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Environment

Create `.env` file:

```bash
# App
APP_NAME=RAG-based Chatbot
APP_ENV=development
APP_PORT=8000

# PostgreSQL
DATABASE_URL=postgresql://user:password@host:port/database

# Redis
REDIS_HOST=your-redis-host
REDIS_PORT=17369
REDIS_PASSWORD=your-password
REDIS_DB=0

# Google Gemini
GEMINI_API_KEY=your-gemini-api-key
GEMINI_MODEL_NAME=gemini-2.5-flash

# Elasticsearch (optional - for hybrid RAG)
ELASTIC_URL=https://your-deployment.es.us-east-1.aws.found.io:9243
ELASTIC_INDEX=company_policies

# Pinecone (optional - for hybrid RAG)
PINECONE_API_KEY=your-pinecone-api-key
PINECONE_ENV=us-east-1
PINECONE_INDEX=company-policies

# OpenAI (optional - for embeddings)
OPENAI_API_KEY=your-openai-api-key
EMBEDDING_MODEL=text-embedding-3-small

# Memory
MAX_SESSION_MESSAGES=200
SESSION_TTL_DAYS=30

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/app_logs.log
```

### 3. Set Up Cloud Services (Optional)

**For hybrid RAG functionality**, set up Elasticsearch and Pinecone:

ğŸ“– **Follow**: [CLOUD_SETUP_GUIDE.md](CLOUD_SETUP_GUIDE.md) for step-by-step instructions

Or use the quick reference: [QUICK_SETUP_REFERENCE.md](QUICK_SETUP_REFERENCE.md)

**Test connections**:
```bash
python test_cloud_connections.py
```

### 4. Index Documents (Optional)

**For hybrid RAG with your own documents**:

```bash
# Add documents to the data folder
mkdir -p kb_pipeline/data/raw
# Copy your PDF, DOCX, TXT, MD files here

# Install KB dependencies
pip install elasticsearch pinecone-client openai PyPDF2 python-docx

# Run indexing
python -m kb_pipeline.pipeline --mode index --data_dir kb_pipeline/data/raw

# Test search
python -m kb_pipeline.pipeline --mode search --query "What is the remote work policy?"
```

### 5. Run the Application

```bash
# Start server
uvicorn app.main:app --reload --port 8000

# Server will be available at http://localhost:8000
```

### 6. Test the System

```bash
# Test orchestrator
python -m tests.test_new_orchestrator

# Test LLM client
python -m tests.test_llm
```

---

## ğŸ“¡ API Usage

### Standard Chat

```bash
curl -X POST http://localhost:8000/api/chat/ \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is the remote work policy?",
    "user_id": "user123"
  }'
```

**Response**:
```json
{
  "session_id": "abc123",
  "reply": "Our remote work policy allows employees to work remotely up to 3 days per week...",
  "classification": "policy-related",
  "retrieved_docs": 3,
  "success": true
}
```

### Streaming Chat

```bash
curl -X POST http://localhost:8000/api/chat/stream \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Tell me about vacation policies",
    "user_id": "user123"
  }'
```

### Get History

```bash
curl http://localhost:8000/api/chat/history/{session_id}
```

### Interactive API Docs

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

---

## ğŸ§ª Testing

### Test Semantic Chunking

```bash
python test_semantic_chunking.py
```

### Test Cloud Connections

```bash
python test_cloud_connections.py
```

### Test Orchestrator

```bash
python -m tests.test_new_orchestrator
```

**Tests include**:
- Policy-related query
- Off-topic query
- Streaming response
- Multi-turn conversation

---

## ğŸ¯ Usage Modes

### Mode 1: Placeholder (Default)

Uses hardcoded example policies - good for testing without cloud setup:

```python
# app/orchestrator/agents/retriever_agent.py
retriever_agent = RetrieverAgent(use_hybrid=False)
```

### Mode 2: Hybrid RAG (Production)

Uses Elasticsearch + Pinecone for real document retrieval:

```python
# After indexing your documents
retriever_agent = RetrieverAgent(use_hybrid=True)
```

---

## ğŸ“Š System Components

### 1. API Layer (FastAPI)
- **Location**: `app/api/chat_api.py`
- **Endpoints**: `/chat/`, `/chat/stream`, `/chat/history/{session_id}`

### 2. Orchestrator (LangGraph)
- **Location**: `app/orchestrator/orchestrator.py`
- **State**: `app/orchestrator/state.py`
- **Workflow**: START â†’ DomainGuard â†’ Retriever â†’ Summarizer â†’ END

### 3. Agents
- **DomainGuard**: Classifies queries as policy-related or off-topic
- **RetrieverAgent**: Hybrid RAG retrieval (BM25 + semantic)
- **SummarizerAgent**: Generates responses with context

### 4. Knowledge Base Pipeline
- **Ingestion**: `kb_pipeline/preprocessor/ingest.py`
- **Preprocessing**: `kb_pipeline/preprocessor/preprocess.py`
- **Sparse Indexing**: `kb_pipeline/indexing/index_sparse.py`
- **Dense Indexing**: `kb_pipeline/indexing/index_dense.py`
- **Hybrid Retrieval**: `kb_pipeline/retrieval/hybrid_retriever.py`
- **Reranker**: `kb_pipeline/retrieval/reranker.py`

### 5. Memory Management
- **Redis STM**: `app/memory/short_term_memory.py` (30-day TTL, 200 messages max)
- **PostgreSQL LTM**: `app/models/conversation.py` (permanent storage)

---

## ğŸ”§ Configuration

### Chunk Size (Semantic)

Edit `kb_pipeline/preprocessor/preprocess.py`:

```python
DocumentPreprocessor(
    target_tokens=350,      # Target 300-400 tokens
    max_tokens=450,         # Force split if exceeded
    overlap_tokens=50,      # Continuity overlap
    min_tokens=50           # Minimum valid chunk
)
```

### Retrieval Weights

```python
# More keyword-focused
retriever = HybridRetriever(sparse_weight=0.7, dense_weight=0.3)

# More semantic-focused
retriever = HybridRetriever(sparse_weight=0.3, dense_weight=0.7)

# Balanced (default)
retriever = HybridRetriever(sparse_weight=0.5, dense_weight=0.5)
```

### Reranking

```python
# LLM reranking (higher quality)
reranker = Reranker(use_llm=True)

# Heuristic reranking (faster, cheaper)
reranker = Reranker(use_llm=False)
```

---

## ğŸ“ Project Structure

```
RAG-based Chatbot/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                    # FastAPI endpoints
â”‚   â”œâ”€â”€ orchestrator/           # LangGraph workflow
â”‚   â”‚   â”œâ”€â”€ agents/             # DomainGuard, Retriever, Summarizer
â”‚   â”‚   â”œâ”€â”€ orchestrator.py     # Main workflow
â”‚   â”‚   â””â”€â”€ state.py            # State schema
â”‚   â”œâ”€â”€ models/                 # PostgreSQL models
â”‚   â”œâ”€â”€ schemas/                # Pydantic schemas
â”‚   â”œâ”€â”€ memory/                 # Redis STM
â”‚   â”œâ”€â”€ config/                 # Settings & DB
â”‚   â”œâ”€â”€ utils/                  # LLM client, logger, Redis
â”‚   â””â”€â”€ main.py                 # FastAPI app
â”œâ”€â”€ kb_pipeline/
â”‚   â”œâ”€â”€ preprocessor/           # Document ingestion & chunking
â”‚   â”œâ”€â”€ indexing/               # Elasticsearch & Pinecone
â”‚   â”œâ”€â”€ retrieval/              # Hybrid retriever & reranker
â”‚   â”œâ”€â”€ pipeline.py             # Main orchestrator
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ tests/                      # Test files
â”œâ”€â”€ logs/                       # Application logs
â”œâ”€â”€ .env                        # Environment variables
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

---

## ğŸ’° Cost Estimates

| Service | Free Tier | Usage | Monthly Cost |
|---------|-----------|-------|--------------|
| **Pinecone** | 100K vectors | 10K vectors | Free |
| **Elasticsearch** | 14-day trial | N/A | $50-100 (smallest) |
| **OpenAI** | $5 credits | ~10K chunks | ~$0.10 |
| **Gemini** | Free tier | API calls | Free |

**Tips to minimize costs**:
- Use free tiers for testing
- Delete cloud resources when not using
- Set strict usage limits
- Consider local alternatives (see below)

---

## ğŸ”„ Alternatives (Cost-Saving)

### Free/Local Alternatives

| Component | Current | Alternative |
|-----------|---------|-------------|
| **Elasticsearch** | Cloud ($50-100/mo) | Local Elasticsearch (free) |
| **Pinecone** | Cloud ($70/mo) | Qdrant, Weaviate, ChromaDB (free) |
| **OpenAI Embeddings** | API ($0.13/1M tokens) | Sentence Transformers (free) |

---

## ğŸ› Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| Import errors | Activate virtual environment, install dependencies |
| Database connection failed | Check DATABASE_URL in `.env` |
| Redis connection failed | Verify Redis host, port, password |
| Elasticsearch auth failed | Check URL includes `https://` and `:9243` |
| Pinecone index not found | Create index in Pinecone dashboard |
| OpenAI quota exceeded | Add payment method, check usage limits |

### Check Logs

```bash
# View all logs
tail -f logs/app_logs.log

# Filter by component
tail -f logs/app_logs.log | grep "RetrieverAgent"
```

---

## ğŸ“š Documentation Links

- **LangGraph**: https://langchain-ai.github.io/langgraph/
- **Pinecone**: https://docs.pinecone.io/
- **Elasticsearch**: https://www.elastic.co/guide/
- **OpenAI**: https://platform.openai.com/docs
- **FastAPI**: https://fastapi.tiangolo.com/

---

## ğŸ¤ Contributing

This is an assignment project. For suggestions or issues, please contact the maintainer.

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ‰ Acknowledgments

Built for the Softvence Agency assignment using:
- Google Gemini for LLM
- LangGraph for orchestration
- Elasticsearch for BM25 search
- Pinecone for semantic search
- FastAPI for REST API

---

## ğŸ“ Support

For issues or questions:
1. Check the documentation guides above
2. Review logs: `logs/app_logs.log`
3. Test individual components
4. Refer to troubleshooting section

---

**Ready to get started?** ğŸš€

1. Follow [CLOUD_SETUP_GUIDE.md](CLOUD_SETUP_GUIDE.md) to set up cloud services
2. Use [test_cloud_connections.py](test_cloud_connections.py) to verify setup
3. Index your documents with the KB pipeline
4. Start the server and test!
